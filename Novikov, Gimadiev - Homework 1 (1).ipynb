{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " # Домашнее задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание выполняли: Новиков Лев и Гимадиев Азат, группа ИАД-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы были проанализированы два типа текстов - художественный, на примере книги В. Распутина \"Уроки французского\", и публицистический, на примере множества статей в издании \"Коммерсант\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation\n",
    "exclude = set(punctuation + '0123456789'+u'–—'+u'«»' + '—'+u'.')\n",
    "# исключаем все возможные знаки препинания, чтобы мы смогли разбить текст на токены, которые сможем анализировать и группировать\n",
    "\n",
    "merged_text1 = open('C:\\\\Users\\\\Лев\\\\Downloads\\\\Minor\\\\2016-2017\\\\1 дз\\\\1.txt', encoding='utf-8').readlines()\n",
    "# merged_text1 = open('C:\\\\Users\\\\Азат\\\\Desktop\\\\4 half-year\\\\texts\\\\1.txt', encoding='utf-8').readlines()\n",
    "merged_text1 = ' '.join(merged_text1)\n",
    "merged_text1 = ''.join(ch for ch in merged_text1 if ch not in exclude) # очистка первого текста (художественный)\n",
    "\n",
    "merged_text2 = open('C:\\\\Users\\\\Лев\\\\Downloads\\\\Minor\\\\2016-2017\\\\1 дз\\\\2.txt', encoding='utf-8').readlines()\n",
    "# merged_text2 = open('C:\\\\Users\\\\Азат\\\\Desktop\\\\4 half-year\\\\texts\\\\2.txt', encoding='utf-8').readlines()\n",
    "merged_text2 = ' '.join(merged_text2)\n",
    "merged_text2 = ''.join(ch for ch in merged_text2 if ch not in exclude) # очистка второго текста (публицистический)\n",
    "\n",
    "# print(merged_text1)\n",
    "# print(merged_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation) # то, что исключали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿странно\n",
      "почему\n",
      "мы\n",
      "так\n",
      "же\n",
      "как\n",
      "и\n",
      "перед\n",
      "родителями\n",
      "всякий\n",
      "\n",
      "\n",
      "﻿лауреаты\n",
      "президентской\n",
      "премии\n",
      "подъехали\n",
      "к\n",
      "первому\n",
      "корпусу\n",
      "кремля\n",
      "на\n",
      "автобусе\n"
     ]
    }
   ],
   "source": [
    "tokens1 = WhitespaceTokenizer().tokenize(merged_text1.lower())\n",
    "for i in tokens1[:10]: print(i)\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "tokens2 = WhitespaceTokenizer().tokenize(merged_text2.lower())\n",
    "for i in tokens2[:10]: print(i)\n",
    "    \n",
    "# проверяем, как прошла очистка, избавляемся от ненужного регистра (поскольку слова Странно и странно это одно и то же)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in first collection: 8902\n",
      "Number of types in first collection: 3244\n",
      "\n",
      "\n",
      "Number of tokens in second collection: 5046\n",
      "Number of types in second collection: 2506\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens in first collection:', len(tokens1))\n",
    "types1 = nltk.FreqDist(tokens1)\n",
    "print('Number of types in first collection:', len(types1)) # слова белый и белого одно один и тот же тип слова, но разные токены\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('Number of tokens in second collection:', len(tokens2))\n",
    "types2 = nltk.FreqDist(tokens2)\n",
    "print('Number of types in second collection:', len(types2)) # аналогично со вторым текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer() # будем использовать морфологический процессор pymorhy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = []\n",
    "for word in merged_text1.split():\n",
    "    # print(word)\n",
    "    # print('Gram info:', morph.parse(word)[0].tag)\n",
    "    # print('POS tag:', morph.parse(word)[0].tag.POS)\n",
    "    table1.append([word, morph.parse(word)[0].tag.POS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_of_speech = ['VERB', 'NPRO', 'PRTF', 'GRND', 'ADVB', 'INFN', 'CONJ', 'ADJS', 'PRTS', 'NOUN', 'PRED', 'COMP', 'ADJF', 'INTJ', 'PREP', 'PRCL', 'NUMR']\n",
    "len(parts_of_speech) # всего есть 17 разных частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1404  980   64  118  685  296 1015   56   24 1784   74   78  739   15  960\n",
      "  555   46]\n",
      "[ 0.15771737  0.11008762  0.0071894   0.01325545  0.076949    0.03325095\n",
      "  0.11401932  0.00629072  0.00269602  0.2004044   0.00831274  0.00876208\n",
      "  0.08301505  0.00168501  0.10784093  0.06234554  0.00516738]\n"
     ]
    }
   ],
   "source": [
    "freq1 = np.zeros(len(parts_of_speech), dtype=np.int)\n",
    "for i1 in range(0, len(table1)):\n",
    "    for j1 in range(0, len(parts_of_speech)):\n",
    "        if str(table1[i1][1]) == str(parts_of_speech[j1]):\n",
    "            freq1[j1] += 1\n",
    "print(freq1)\n",
    "relfreq1 = freq1/int(len(table1))\n",
    "print(relfreq1) # абсолютная частота - это просто количество вхождений, а нормализованная частота - с учетом общего количества токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VERB', 0.15771736688384633], ['NPRO', 0.11008762075937992], ['PRTF', 0.0071893956414288921], ['GRND', 0.013255448213884521], ['ADVB', 0.076949000224668621], ['INFN', 0.033250954841608629], ['CONJ', 0.11401932150078634], ['ADJS', 0.0062907211862502808], ['PRTS', 0.0026960233655358348], ['NOUN', 0.20040440350483038], ['PRED', 0.0083127387104021573], ['COMP', 0.0087620759379914634], ['ADJF', 0.083015052797124247], ['INTJ', 0.0016850146034598967], ['PREP', 0.10784093462143339], ['PRCL', 0.062345540328016175], ['NUMR', 0.0051673781172770165]]\n"
     ]
    }
   ],
   "source": [
    "Freqdist1 = []\n",
    "for l in range(len(relfreq1)):\n",
    "    Freqdist1.append([parts_of_speech[l], relfreq1[l]])\n",
    "print(Freqdist1) # импровизированный словарь для частей речи и их относительных частот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = []\n",
    "for word in merged_text2.split():\n",
    "    # print(word)\n",
    "    # print('Gram info:', morph.parse(word)[0].tag)\n",
    "    # print('POS tag:', morph.parse(word)[0].tag.POS)\n",
    "    table2.append([word, morph.parse(word)[0].tag.POS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 528  208   73   12  248  139  463   43   37 1735   32   14  612    5  593\n",
      "  223   22]\n",
      "[ 0.10463734  0.04122077  0.0144669   0.00237812  0.04914784  0.02754657\n",
      "  0.09175585  0.0085216   0.00733254  0.3438367   0.00634166  0.00277447\n",
      "  0.12128419  0.00099088  0.11751883  0.04419342  0.00435989]\n"
     ]
    }
   ],
   "source": [
    "freq2 = np.zeros(len(parts_of_speech), dtype=np.int)\n",
    "for i2 in range(0, len(table2)):\n",
    "    for j2 in range(0, len(parts_of_speech)):\n",
    "        if str(table2[i2][1]) == str(parts_of_speech[j2]):\n",
    "            freq2[j2] += 1\n",
    "print(freq2)\n",
    "relfreq2 = freq2/int(len(table2))\n",
    "print(relfreq2) # аналогично считаем для второго текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VERB', 0.10463733650416171], ['NPRO', 0.04122076892588189], ['PRTF', 0.014466904478795084], ['GRND', 0.0023781212841854932], ['ADVB', 0.049147839873166864], ['INFN', 0.0275465715418153], ['CONJ', 0.091755846214823619], ['ADJS', 0.0085216012683313521], ['PRTS', 0.0073325406262386046], ['NOUN', 0.34383670233848596], ['PRED', 0.0063416567578279829], ['COMP', 0.0027744748315497426], ['ADJF', 0.12128418549346016], ['INTJ', 0.00099088386841062219], ['PREP', 0.1175188267934998], ['PRCL', 0.044193420531113754], ['NUMR', 0.0043598890210067376]]\n"
     ]
    }
   ],
   "source": [
    "Freqdist2 = []\n",
    "for l in range(len(relfreq2)):\n",
    "    Freqdist2.append([parts_of_speech[l], relfreq2[l]])\n",
    "print(Freqdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.spearman import *\n",
    "print(spearman_correlation(ranks_from_sequence(freq1), ranks_from_sequence(freq2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Результаты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были составлены частотные словари для относительных частот появления по частям речи для текстов в двух стилях, художественном и публицистическом, и посчитан коэффициент корреляции Спирмена. Видно, что в обоих словарях на первом месте по частоте употребления находятся существительные, а на втором - глаголы, но несмотря на это, коэффициент корреляции равен нулю.\n",
    "\n",
    "Поэтому гипотеза о том, что в разных стилях разные части речи употребляются с различной частотой, верна (как можно видеть, у нас в одном тексте существительные встречаются втрое чаще, чем глаголы, а в другом едва ли в полтора раза чаще), тексты отличаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
